# üß™ Gu√≠a para Probar TinyLlama

## ‚úÖ ¬°Ahora funciona SIN Ollama!

**TinyLlama ahora usa `@xenova/transformers`** para ejecutar el modelo directamente en Node.js, sin necesidad de Ollama.

## Prerequisitos

**¬°Ya no necesitas Ollama!** El servicio funciona con:
- ‚úÖ `@xenova/transformers` (ya instalado en el proyecto)
- ‚úÖ Conexi√≥n a internet (la primera vez, para descargar el modelo ~637 MB)
- ‚úÖ Node.js con suficiente memoria (al menos 4GB RAM recomendado)

**Nota:** La primera vez que ejecutes el servicio, el modelo se descargar√° autom√°ticamente desde Hugging Face y se guardar√° localmente para uso futuro.

## C√≥mo Probar

### Opci√≥n 1: Script de Prueba Autom√°tico (RECOMENDADO) üöÄ

El m√©todo m√°s f√°cil para verificar que TinyLlama funciona:

```bash
npm run test:tinyllama
```

Este script (escrito en TypeScript):
- ‚úÖ Carga el modelo TinyLlama usando @xenova/transformers
- ‚úÖ **NO requiere Ollama** - funciona completamente standalone
- ‚úÖ Env√≠a un mensaje de prueba al modelo
- ‚úÖ Muestra la respuesta generada por TinyLlama

**Nota:** La primera ejecuci√≥n descargar√° el modelo (~637 MB), puede tardar varios minutos dependiendo de tu conexi√≥n.

### Opci√≥n 2: Usando el archivo HTTP (REST Client en VS Code)

1. Abre el archivo: `src/message/https/messages.http`
2. Ve a la secci√≥n "üß™ 6. PRUEBA RESPUESTA TINY LLAMA" (l√≠nea 67)
3. Aseg√∫rate de que:
   - El servidor NestJS est√© corriendo (`npm run start:dev`)
   - Tengas un token JWT v√°lido (el que est√° en el archivo puede estar expirado)
   - **NO necesitas Ollama** - el servicio usa transformers.js directamente
4. Haz clic en "Send Request" sobre la l√≠nea del POST

### Opci√≥n 3: Usando curl

```bash
curl -X POST http://localhost:3000/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer TU_TOKEN_JWT_AQUI" \
  -d "{\"chatId\": 1, \"contenido\": \"Hola TinyLlama, ¬øpuedes ayudarme?\"}"
```

## Respuesta Esperada

Si todo funciona correctamente, deber√≠as recibir una respuesta como:

```json
{
  "mensajeUsuario": {
    "id": 123,
    "content": "Tiny, ¬øpuedes responderme?",
    "sentimiento": "NEGATIVO",
    "isBot": false,
    ...
  },
  "mensajeBot": {
    "id": 124,
    "content": "Lamento escuchar que te sientes as√≠...",
    "isBot": true,
    ...
  }
}
```

## Soluci√≥n de Problemas

### Error: "Cannot load model" o errores de descarga
- **Verifica tu conexi√≥n a internet**: El modelo se descarga la primera vez (~637 MB)
- **Espacio en disco**: Aseg√∫rate de tener al menos 1GB libres
- **Memoria**: El modelo requiere al menos 4GB de RAM disponible
- **Primera descarga**: La primera vez puede tardar varios minutos, ten paciencia
- **Cache**: El modelo se guarda en `~/.cache/huggingface/hub/` para uso futuro

### El modelo carga lentamente
- **Es normal**: La primera carga del modelo puede tardar 30-60 segundos
- **Carga en memoria**: El modelo se carga en RAM al iniciar el servidor
- **Recomendaci√≥n**: Si tienes GPU CUDA, puedes cambiar `device: 'cpu'` a `device: 'gpu'` en el c√≥digo

### Error: "Out of memory"
- **Reduce el modelo**: Ya usa quantizaci√≥n (q8), pero si persiste:
  - Cierra otras aplicaciones que consuman memoria
  - Considera usar un modelo m√°s peque√±o
- **Alternativa**: El servicio tiene un fallback inteligente que funciona sin el modelo

### Error: "JWT expired" o "Unauthorized"
- Necesitas un token JWT v√°lido. Inicia sesi√≥n primero:
  ```bash
  POST http://localhost:3000/auth/login
  {
    "email": "tu@email.com",
    "password": "tu_password"
  }
  ```

### El servidor NestJS no responde
- Verifica que est√© corriendo: `npm run start:dev`
- Revisa los logs del servidor: Busca mensajes como "Modelo TinyLlama cargado correctamente"
- Si el modelo no carga: El servicio usar√° un fallback inteligente autom√°ticamente

### Desactivar el modelo y usar solo fallback
Si quieres desactivar completamente el modelo y usar solo el an√°lisis heur√≠stico:
- Agrega a tu `.env`: `USE_TRANSFORMERS=false`
- Esto usar√° solo an√°lisis de sentimiento basado en palabras clave

