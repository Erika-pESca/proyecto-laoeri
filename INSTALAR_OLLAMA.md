#  C贸mo Instalar Ollama en Windows

Ollama es necesario para que TinyLlama funcione. Aqu铆 tienes las opciones para instalarlo:

## Opci贸n 1: Instalaci贸n Manual de Ollama (RECOMENDADO)

### Paso 1: Descargar Ollama
1. Ve a: **https://ollama.ai/download**
2. Descarga el instalador para Windows (`.exe`)
3. Ejecuta el instalador y sigue las instrucciones

### Paso 2: Verificar la instalaci贸n
Abre una **nueva terminal PowerShell** y ejecuta:

```powershell
ollama --version
```

Deber铆as ver algo como: `ollama version is 1.x.x`

### Paso 3: Instalar el modelo TinyLlama
```powershell
ollama pull tinyllama
```

Esto descargar谩 el modelo (aprox. 637 MB). Puede tardar unos minutos dependiendo de tu conexi贸n.

### Paso 4: Verificar que el modelo est茅 instalado
```powershell
ollama list
```

Deber铆as ver `tinyllama` en la lista.

### Paso 5: Iniciar Ollama (si no est谩 como servicio)
Si Ollama no se inicia autom谩ticamente como servicio, in铆cialo manualmente:

```powershell
ollama serve
```

O simplemente abre la aplicaci贸n Ollama desde el men煤 de inicio.

### Paso 6: Probar que funciona
```powershell
cd MindConnectAI
npm run test:tinyllama
```

---

## Opci贸n 2: Usando Docker (si tienes Docker Desktop instalado)

### Paso 1: Instalar Docker Desktop
Si no lo tienes, desc谩rgalo desde: **https://www.docker.com/products/docker-desktop/**

### Paso 2: Iniciar Ollama en Docker
```powershell
docker run -d -p 11434:11434 --name ollama ollama/ollama
```

### Paso 3: Instalar TinyLlama en el contenedor
```powershell
docker exec -it ollama ollama pull tinyllama
```

### Paso 4: Verificar
```powershell
docker exec -it ollama ollama list
```

---

## Opci贸n 3: Instalaci贸n R谩pida desde PowerShell (con winget)

Si tienes Windows Package Manager (winget) instalado:

```powershell
winget install Ollama.Ollama
```

Luego sigue los pasos 3-6 de la Opci贸n 1.

---

## Verificar que Ollama est谩 corriendo

Una vez instalado, verifica que Ollama est茅 corriendo accediendo a:

**http://localhost:11434/api/tags**

En tu navegador deber铆as ver un JSON con los modelos instalados.

---

## Soluci贸n de Problemas

### "Ollama no est谩 corriendo"
- Verifica que el servicio de Ollama est茅 activo en el Administrador de Tareas
- O inicia Ollama manualmente: `ollama serve`
- Verifica que el puerto 11434 no est茅 en uso por otro programa

### "Model 'tinyllama' not found"
- Ejecuta: `ollama pull tinyllama`
- Verifica: `ollama list`

### El script de prueba sigue fallando
1. Abre una nueva terminal PowerShell (para refrescar las variables de entorno)
2. Verifica: `ollama --version`
3. Verifica que Ollama est茅 corriendo: Abre http://localhost:11434/api/tags en el navegador
4. Ejecuta el script nuevamente: `npm run test:tinyllama`

---

## Recursos

- **Sitio oficial**: https://ollama.ai
- **Documentaci贸n**: https://github.com/ollama/ollama
- **Modelos disponibles**: https://ollama.ai/library

